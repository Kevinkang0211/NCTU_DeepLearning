{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "y_train = np.array(pd.get_dummies(x_train.Category))\n",
    "#labels = pd.factorize(x_train.Category)\n",
    "x_train = x_train.drop(['Id','Category'],axis=1)\n",
    "\n",
    "x_test = pd.read_csv(\"test.csv\")\n",
    "x_test = x_test.drop(['Id'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Text Preprocessing 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.本次文字前處裡採用NLTK套件，作為Tokenizer的工具；且針對空白字元，我們採取忽略不tokenize它們\n",
    "\n",
    "2.加入special token的原因是因為每個標題長度要一致，所以我們要用 PAD 來填充過短的標題；另外如果字詞沒在字典裡出現過，就用 UNK 的替代它\n",
    "\n",
    "3.Text Preprocessing的程序為: \n",
    "\n",
    "讓標題轉為小寫 --> 利用Regex去除標點符號 --> 去除white spaces -->用NLTK token文字 --> 去除stop word --> 用NLTK stemming 文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "titles = x_train['Title'].copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    #讓標題轉成小寫\n",
    "    titles[i] = titles[i].lower()\n",
    "    \n",
    "    #去除標點符號 Removing punctuations Using regex\n",
    "    titles[i] = re.sub(r'[^\\w\\s]', '', titles[i])\n",
    "    \n",
    "    \n",
    "    #Remove whitespaces, remove leading and ending spaces\n",
    "    titles[i] = titles[i].strip()\n",
    "    \n",
    "    #用NLTK token文字\n",
    "    titles[i] = word_tokenize(titles[i])\n",
    "    \n",
    "    #去除stop word\n",
    "    titles[i] = [i for i in titles[i] if not i in stop_words]\n",
    "    \n",
    "x_train['Title'] = titles\n",
    "\n",
    "#stemming處理\n",
    "for i in range(len(x_train['Title'])):\n",
    "    templist=[]\n",
    "    for w in x_train['Title'][i]:\n",
    "        templist.append(ps.stem(w))\n",
    "    x_train['Title'][i] = templist\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#x_test 前處理\n",
    "titles = x_test['Title'].copy()\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    #讓標題轉成小寫\n",
    "    titles[i] = titles[i].lower()\n",
    "    \n",
    "    #去除標點符號 Removing punctuations Using regex\n",
    "    titles[i] = re.sub(r'[^\\w\\s]', '', titles[i])\n",
    "    \n",
    "    \n",
    "    #Remove whitespaces, remove leading and ending spaces\n",
    "    titles[i] = titles[i].strip()\n",
    "    \n",
    "    #用NLTK token文字\n",
    "    titles[i] = word_tokenize(titles[i])\n",
    "    \n",
    "    #去除stop word\n",
    "    titles[i] = [i for i in titles[i] if not i in stop_words]\n",
    "    \n",
    "x_test['Title'] = titles\n",
    "\n",
    "\n",
    "#stemming處理\n",
    "for i in range(len(x_test['Title'])):\n",
    "    templist=[]\n",
    "    for w in x_test['Title'][i]:\n",
    "        templist.append(ps.stem(w))\n",
    "    x_test['Title'][i] = templist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[roddick, talk, new, coach]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[prodigi, join, v, festiv, lineup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sundanc, honour, foreign, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[dunn, keen, commit, man, citi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[row, polic, power, cso]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>[lufthansa, may, sue, bush, visit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>[roll, next, gener, net]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>[mirza, make, indian, tenni, histori]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>[gta, sequel, crimin, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>[goahead, new, internet, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title\n",
       "0               [roddick, talk, new, coach]\n",
       "1        [prodigi, join, v, festiv, lineup]\n",
       "2          [sundanc, honour, foreign, film]\n",
       "3           [dunn, keen, commit, man, citi]\n",
       "4                  [row, polic, power, cso]\n",
       "...                                     ...\n",
       "1775     [lufthansa, may, sue, bush, visit]\n",
       "1776               [roll, next, gener, net]\n",
       "1777  [mirza, make, indian, tenni, histori]\n",
       "1778            [gta, sequel, crimin, good]\n",
       "1779         [goahead, new, internet, name]\n",
       "\n",
       "[1780 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[winemak, reject, foster, offer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[boe, unveil, new, 777, aircraft]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brown, visit, slum, africa, trip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[us, blogger, fire, airlin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mansfield, 01, leyton, orient]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>[queen, recruit, singer, new, tour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>[slim, playstat, tripl, sale]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>[vera, drake, bafta, triumph, hope]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>[tindal, want, second, opinion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>[text, messag, record, smash]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title\n",
       "0       [winemak, reject, foster, offer]\n",
       "1      [boe, unveil, new, 777, aircraft]\n",
       "2     [brown, visit, slum, africa, trip]\n",
       "3            [us, blogger, fire, airlin]\n",
       "4        [mansfield, 01, leyton, orient]\n",
       "..                                   ...\n",
       "440  [queen, recruit, singer, new, tour]\n",
       "441        [slim, playstat, tripl, sale]\n",
       "442  [vera, drake, bafta, triumph, hope]\n",
       "443      [tindal, want, second, opinion]\n",
       "444        [text, messag, record, smash]\n",
       "\n",
       "[445 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = Tokenizer(num_words=6000)  \n",
    "\n",
    "#使用Tokenizer模組建立token，建立一個6000字的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.fit_on_texts(x_train['Title'])\n",
    "token.fit_on_texts(x_test['Title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = token.texts_to_sequences(x_train['Title'])\n",
    "x_test_seq = token.texts_to_sequences(x_test['Title'])\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_seq,maxlen=20)\n",
    "x_test = sequence.pad_sequences(x_test_seq,maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.2 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【Word Embedding】\n",
    "\n",
    "-這次使用Pre-trained的 word embedding是使用 Glove.6B.200d檔\n",
    "\n",
    "-會使用pre-trained的word embedding的原因是因為，這些像是glove、w2v、fasttext都是研究機構或是企業已經將好幾百萬、千萬的文章、文字都看過，並編成的word vector，所以有點像是站在巨人的肩膀上一樣，可以將這些已經學過、訓練過的文字向量作為我們預設的向量，而不是一開始隨機預設然後再慢慢學習。\n",
    "\n",
    "-將每個標題轉為Keras可讀的形式，在截長補短padding到長度為20\n",
    "\n",
    "【Model Design】\n",
    "\n",
    "-本次任務使用了 GRU 作為預測模型\n",
    "\n",
    "-模型架構為: 64個神經元的GRU -> 32個神經元的隱藏層 採用relu作為activation function -> 最後一層為softmax 5個神經元的輸出\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "#這邊需要自行下載glove檔案到本機路徑\n",
    "f = open(os.path.join('C:/Users/kevin/glove.6B', 'glove.6B.200d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = token.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGRU = Sequential() #建立模型\n",
    "\n",
    "modelGRU.add(Embedding(len(word_index) + 1,\n",
    "                            200,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=20,\n",
    "                            trainable=False))\n",
    "\n",
    "\n",
    "modelGRU.add(Dropout(0.2)) #隨機在神經網路中放棄神經元，避免overfitting\n",
    "\n",
    "#建立64個神經元的GRU層\n",
    "\n",
    "modelGRU.add(GRU(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelGRU.add(Dense(units=32,activation='relu'))\n",
    "modelGRU.add(Dropout(0.2))\n",
    "#建立一個輸出層\n",
    "modelGRU.add(Dense(units=5,activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#設定EARLY STOP機制 監控loss變化\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1246 samples, validate on 534 samples\n",
      "Epoch 1/50\n",
      "1246/1246 [==============================] - 1s 702us/step - loss: 1.5302 - accuracy: 0.3363 - val_loss: 1.3362 - val_accuracy: 0.5824\n",
      "Epoch 2/50\n",
      "1246/1246 [==============================] - 0s 349us/step - loss: 1.1913 - accuracy: 0.5738 - val_loss: 0.9625 - val_accuracy: 0.6835\n",
      "Epoch 3/50\n",
      "1246/1246 [==============================] - 0s 348us/step - loss: 0.8832 - accuracy: 0.6806 - val_loss: 0.7772 - val_accuracy: 0.7285\n",
      "Epoch 4/50\n",
      "1246/1246 [==============================] - 0s 357us/step - loss: 0.7666 - accuracy: 0.7416 - val_loss: 0.7023 - val_accuracy: 0.7528\n",
      "Epoch 5/50\n",
      "1246/1246 [==============================] - 0s 360us/step - loss: 0.6564 - accuracy: 0.7689 - val_loss: 0.6615 - val_accuracy: 0.7622\n",
      "Epoch 6/50\n",
      "1246/1246 [==============================] - 0s 370us/step - loss: 0.5884 - accuracy: 0.8034 - val_loss: 0.6096 - val_accuracy: 0.7865\n",
      "Epoch 7/50\n",
      "1246/1246 [==============================] - 0s 361us/step - loss: 0.5288 - accuracy: 0.8218 - val_loss: 0.6064 - val_accuracy: 0.7959\n",
      "Epoch 8/50\n",
      "1246/1246 [==============================] - 0s 357us/step - loss: 0.4729 - accuracy: 0.8307 - val_loss: 0.5939 - val_accuracy: 0.7978\n",
      "Epoch 9/50\n",
      "1246/1246 [==============================] - 0s 352us/step - loss: 0.4677 - accuracy: 0.8250 - val_loss: 0.5890 - val_accuracy: 0.8052\n",
      "Epoch 10/50\n",
      "1246/1246 [==============================] - 0s 375us/step - loss: 0.4251 - accuracy: 0.8499 - val_loss: 0.5902 - val_accuracy: 0.8015\n",
      "Epoch 11/50\n",
      "1246/1246 [==============================] - 0s 369us/step - loss: 0.4095 - accuracy: 0.8620 - val_loss: 0.6034 - val_accuracy: 0.7996\n",
      "Epoch 12/50\n",
      "1246/1246 [==============================] - 0s 362us/step - loss: 0.3473 - accuracy: 0.8844 - val_loss: 0.6007 - val_accuracy: 0.8034\n",
      "Epoch 13/50\n",
      "1246/1246 [==============================] - 0s 368us/step - loss: 0.3504 - accuracy: 0.8804 - val_loss: 0.5856 - val_accuracy: 0.8071\n",
      "Epoch 14/50\n",
      "1246/1246 [==============================] - 0s 358us/step - loss: 0.3762 - accuracy: 0.8676 - val_loss: 0.5818 - val_accuracy: 0.8127\n",
      "Epoch 15/50\n",
      "1246/1246 [==============================] - 0s 353us/step - loss: 0.3257 - accuracy: 0.8812 - val_loss: 0.5727 - val_accuracy: 0.8202\n",
      "Epoch 16/50\n",
      "1246/1246 [==============================] - 0s 353us/step - loss: 0.3244 - accuracy: 0.8949 - val_loss: 0.6065 - val_accuracy: 0.8071\n",
      "Epoch 17/50\n",
      "1246/1246 [==============================] - 0s 359us/step - loss: 0.2933 - accuracy: 0.8909 - val_loss: 0.5984 - val_accuracy: 0.8071\n",
      "Epoch 18/50\n",
      "1246/1246 [==============================] - 0s 363us/step - loss: 0.2787 - accuracy: 0.9149 - val_loss: 0.5862 - val_accuracy: 0.8146\n",
      "Epoch 19/50\n",
      "1246/1246 [==============================] - 0s 353us/step - loss: 0.2883 - accuracy: 0.8925 - val_loss: 0.5867 - val_accuracy: 0.8184\n",
      "Epoch 20/50\n",
      "1246/1246 [==============================] - 0s 368us/step - loss: 0.2556 - accuracy: 0.9149 - val_loss: 0.5933 - val_accuracy: 0.8090\n",
      "Epoch 21/50\n",
      "1246/1246 [==============================] - 0s 355us/step - loss: 0.2146 - accuracy: 0.9342 - val_loss: 0.6011 - val_accuracy: 0.8165\n",
      "Epoch 22/50\n",
      "1246/1246 [==============================] - 0s 358us/step - loss: 0.2213 - accuracy: 0.9189 - val_loss: 0.6032 - val_accuracy: 0.8127\n",
      "Epoch 23/50\n",
      "1246/1246 [==============================] - 0s 360us/step - loss: 0.2123 - accuracy: 0.9278 - val_loss: 0.6050 - val_accuracy: 0.8165\n",
      "Epoch 24/50\n",
      "1246/1246 [==============================] - 0s 355us/step - loss: 0.1916 - accuracy: 0.9302 - val_loss: 0.6084 - val_accuracy: 0.8202\n",
      "Epoch 25/50\n",
      "1246/1246 [==============================] - 0s 376us/step - loss: 0.2071 - accuracy: 0.9254 - val_loss: 0.6374 - val_accuracy: 0.8052\n",
      "Epoch 26/50\n",
      "1246/1246 [==============================] - 0s 362us/step - loss: 0.2114 - accuracy: 0.9310 - val_loss: 0.6018 - val_accuracy: 0.8240\n",
      "Epoch 27/50\n",
      "1246/1246 [==============================] - 0s 378us/step - loss: 0.1741 - accuracy: 0.9454 - val_loss: 0.6340 - val_accuracy: 0.8034\n",
      "Epoch 28/50\n",
      "1246/1246 [==============================] - 0s 365us/step - loss: 0.1667 - accuracy: 0.9422 - val_loss: 0.6247 - val_accuracy: 0.8184\n",
      "Epoch 29/50\n",
      "1246/1246 [==============================] - 0s 372us/step - loss: 0.1728 - accuracy: 0.9398 - val_loss: 0.6544 - val_accuracy: 0.8109\n",
      "Epoch 30/50\n",
      "1246/1246 [==============================] - 0s 371us/step - loss: 0.1523 - accuracy: 0.9430 - val_loss: 0.6368 - val_accuracy: 0.8127\n",
      "Epoch 31/50\n",
      "1246/1246 [==============================] - 0s 372us/step - loss: 0.1499 - accuracy: 0.9486 - val_loss: 0.6429 - val_accuracy: 0.8146\n",
      "Epoch 32/50\n",
      "1246/1246 [==============================] - 0s 375us/step - loss: 0.1483 - accuracy: 0.9446 - val_loss: 0.6335 - val_accuracy: 0.8240\n",
      "Epoch 33/50\n",
      "1246/1246 [==============================] - 0s 360us/step - loss: 0.1247 - accuracy: 0.9575 - val_loss: 0.6587 - val_accuracy: 0.8109\n",
      "Epoch 34/50\n",
      "1246/1246 [==============================] - 0s 359us/step - loss: 0.1395 - accuracy: 0.9502 - val_loss: 0.6721 - val_accuracy: 0.8146\n",
      "Epoch 35/50\n",
      "1246/1246 [==============================] - 0s 370us/step - loss: 0.1501 - accuracy: 0.9518 - val_loss: 0.6518 - val_accuracy: 0.8184\n",
      "Epoch 36/50\n",
      "1246/1246 [==============================] - 0s 370us/step - loss: 0.1193 - accuracy: 0.9607 - val_loss: 0.6490 - val_accuracy: 0.8202\n",
      "Epoch 37/50\n",
      "1246/1246 [==============================] - 0s 390us/step - loss: 0.1268 - accuracy: 0.9494 - val_loss: 0.6591 - val_accuracy: 0.8165\n",
      "Epoch 38/50\n",
      "1246/1246 [==============================] - 0s 387us/step - loss: 0.1238 - accuracy: 0.9623 - val_loss: 0.6691 - val_accuracy: 0.8184\n",
      "Epoch 39/50\n",
      "1246/1246 [==============================] - 0s 381us/step - loss: 0.1287 - accuracy: 0.9559 - val_loss: 0.6640 - val_accuracy: 0.8352\n"
     ]
    }
   ],
   "source": [
    "modelGRU.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "train_history = modelGRU.fit(x_train,y_train,\n",
    "epochs=50,\n",
    "batch_size=32,\n",
    "callbacks=[callback],\n",
    "verbose=1,\n",
    "validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#將訓練好的模型儲存起來\n",
    "modelGRU.save('GRU_model')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = modelGRU.predict(x_test, verbose=0)\n",
    "\n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "\n",
    "classes = ['business','entertainment','politics','sport','tech']\n",
    "\n",
    "result =[]\n",
    "for i in range(len(y_predict)):\n",
    "    result.append([i,classes[y_predict[i]]])\n",
    "    \n",
    "final_result = pd.DataFrame(data=result, index=None, columns=['Id','Category'], dtype=None, copy=False)\n",
    "\n",
    "final_result.to_csv(r'309706033_submission_RNN.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將先前儲存的模型載入\n",
    "\n",
    "#reconstructed_model = keras.models.load_model(\"GRU_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "y_predict = reconstructed_model.predict(x_test, verbose=0)\n",
    "\n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "\n",
    "classes = ['business','entertainment','politics','sport','tech']\n",
    "\n",
    "result =[]\n",
    "for i in range(len(y_predict)):\n",
    "    result.append([i,classes[y_predict[i]]])\n",
    "    \n",
    "final_result = pd.DataFrame(data=result, index=None, columns=['Id','Category'], dtype=None, copy=False)\n",
    "\n",
    "final_result.to_csv(r'309706033_submission_RNN.csv', index = False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
